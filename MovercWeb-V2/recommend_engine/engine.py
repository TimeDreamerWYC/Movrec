# ç‹¬ç«‹æ¼”å‘˜æ£€ç´¢ä¸æ¨¡ç³ŠåŒ¹é…å‡½æ•°
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def _normalize_name(s: str):
    """ç®€å•å½’ä¸€åŒ–ï¼šå»é¦–å°¾ç©ºç™½ã€è½¬å°å†™ã€å»æ‰å¸¸è§æ ‡ç‚¹ï¼ˆä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼‰ã€‚"""
    if s is None:
        return ''
    s = str(s).strip()
    # å°å†™åŒ–
    s_low = s.lower()
    # å»æ‰å¸¸è§æ‹‰ä¸æ ‡ç‚¹å’Œæ‹¬å·ä¸­çš„å†…å®¹ï¼ˆä¾‹å¦‚è§’è‰²è¯´æ˜ï¼‰ï¼Œä¿ç•™ä¸­æ–‡ä¸å­—æ¯æ•°å­—ã€ç©ºæ ¼ã€ç‚¹ã€è¿å­—ç¬¦
    s_low = re.sub(r'[\(\)\[\]\{\}<>Â«Â»"\'`ï¼Œ,ã€‚ï¼ï¼Ÿ!?:;â€”\-â€“]', ' ', s_low)
    # å‹ç¼©è¿ç»­ç©ºç™½
    s_low = re.sub(r"\s+", ' ', s_low).strip()
    return s_low


def extract_actors_and_search(movies_df, query, top_k=10):
    """
    è¾“å…¥æ¼”å‘˜åå…³é”®å­—ï¼Œè¿”å›ï¼š
    - 'direct': åŒ…å«å…³é”®å­—çš„æ¼”å‘˜åŸååˆ—è¡¨ï¼ˆä¼˜å…ˆè¿”å›ï¼Œæœ€å¤š top_kï¼‰
    - 'fuzzy': å¦‚æœ direct ä¸ºç©ºï¼Œè¿”å›åŸºäº TF-IDF+ä½™å¼¦ç›¸ä¼¼åº¦çš„æœ€ç›¸ä¼¼æ¼”å‘˜åŸåæˆ– None

    è¯´æ˜ï¼šåŒ¹é…ä½¿ç”¨å½’ä¸€åŒ–å½¢å¼ï¼ˆå°å†™ã€å»æ ‡ç‚¹ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æé«˜å‘½ä¸­ç‡ã€‚
    """
    query = (query or '').strip()
    if not query or movies_df is None or 'ACTORS' not in movies_df.columns:
        return {'direct': [], 'fuzzy': None}

    # æ„å»ºåŸåé›†åˆä¸å½’ä¸€åŒ–æ˜ å°„
    actor_set = set()
    for actors in movies_df['ACTORS'].dropna():
        for name in re.split(r'[\\/;ï¼Œ,ã€\n]+', str(actors)):
            n = name.strip()
            if n:
                actor_set.add(n)
    actor_list = sorted(actor_set)

    # å½’ä¸€åŒ–æ˜ å°„ï¼šnorm -> list(orig)
    norm_map = {}
    norm_list = []
    for a in actor_list:
        na = _normalize_name(a)
        if na in norm_map:
            norm_map[na].append(a)
        else:
            norm_map[na] = [a]
            norm_list.append(na)

    nq = _normalize_name(query)

    # ç›´æ¥åŒ…å«ï¼šåœ¨å½’ä¸€åŒ–åå­—ä¸­æŸ¥æ‰¾åŒ…å«å…³ç³»
    direct_norm_hits = [na for na in norm_list if nq in na]
    direct_hits = []
    for na in direct_norm_hits:
        direct_hits.extend(norm_map.get(na, []))

    # é™åˆ¶ç»“æœæ•°é‡
    direct_hits = direct_hits[:top_k]

    fuzzy_hit = None
    if not direct_hits and norm_list:
        try:
            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,2))
            tfidf = vectorizer.fit_transform(norm_list + [nq])
            sim = cosine_similarity(tfidf[-1], tfidf[:-1]).flatten()
            idx = int(np.argmax(sim))
            if sim[idx] > 0.08:  # æ”¾å®½é˜ˆå€¼ä»¥å…¼å®¹çŸ­è¾“å…¥
                best_norm = norm_list[idx]
                # é€‰æ‹©è¯¥å½’ä¸€åŒ–åå¯¹åº”çš„ç¬¬ä¸€ä¸ªåŸåä½œä¸ºå±•ç¤º
                fuzzy_hit = norm_map.get(best_norm, [None])[0]
        except Exception as e:
            print(f"[ERROR] fuzzy actor match failed: {e}")

    # è°ƒè¯•è¾“å‡º
    if not direct_hits and not fuzzy_hit:
        print(f"[DEBUG] actor search no hits for query='{query}' (normalized='{nq}'), actor_count={len(actor_list)}")

    return {'direct': direct_hits, 'fuzzy': fuzzy_hit}
# æ¼”å‘˜æ¨¡ç³ŠåŒ¹é…å‡½æ•°
def fuzzy_search_by_actor(actor_query, top_n=10):
    """
    æ”¯æŒæ¼”å‘˜åæ¨¡ç³ŠåŒ¹é…ï¼Œè¿”å›ç›¸å…³ç”µå½±åˆ—è¡¨ã€‚
    æŒ‰å½’ä¸€åŒ–æ–‡æœ¬åç”¨ TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…ã€‚
    """
    global movies_new, A, actor_index
    if movies_new is None or A is None or actor_index is None:
        return pd.DataFrame()
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    # å½’ä¸€åŒ–è¾“å…¥
    norm_query = normalize_text(actor_query)
    # æ„é€ å€™é€‰æ¼”å‘˜åˆ—è¡¨
    all_actors = list(actor_index.keys())
    # TF-IDF å‘é‡åŒ–
    tfidf = TfidfVectorizer().fit(all_actors + [norm_query])
    query_vec = tfidf.transform([norm_query])
    actor_vecs = tfidf.transform(all_actors)
    sims = cosine_similarity(query_vec, actor_vecs).flatten()
    # æ‰¾åˆ°æœ€ç›¸è¿‘çš„æ¼”å‘˜
    best_idx = sims.argmax()
    best_actor = all_actors[best_idx]
    # æ‰¾åˆ°æ‰€æœ‰åŒ…å«è¯¥æ¼”å‘˜çš„ç”µå½±
    import numpy as np
    actor_mask = A[:, best_idx] > 0
    matched_movies = movies_new[actor_mask]
    # è¿”å›å‰ top_n éƒ¨ç”µå½±
    return matched_movies.head(top_n).reset_index(drop=True)
def normalize_text(s):
    """ç»Ÿä¸€æ–‡æœ¬æ ¼å¼ï¼šå»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ã€åˆ†éš”ç¬¦ï¼Œè½¬ä¸ºå°å†™ã€‚"""
    if not isinstance(s, str):
        s = str(s)
    # å»é™¤æ‰€æœ‰ç©ºæ ¼ã€æ ‡ç‚¹ã€åˆ†éš”ç¬¦
    s = re.sub(r'[\sÂ·/|,;:._\-]', '', s)
    s = re.sub(r'[\u3000-\u303F\u2000-\u206F\uFF00-\uFFEF]', '', s)  # ä¸­æ–‡ç¬¦å·
    s = s.lower()
    return s
# recommend_engine/engine.py
import pandas as pd
import numpy as np
import jieba
import re
import os
import pickle # ç”¨äºç¼“å­˜é¢„å¤„ç†æ•°æ®å’Œæ¨¡å‹
import logging
import threading

# NOTE: Heavy dependencies (tensorflow, sklearn) are imported lazily inside
# functions to avoid long import times on module import. This allows lightweight
# operations (e.g. checking get_movies_dataframe) without pulling in TF/Scipy.

# --- å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹çŠ¶æ€ ---
# è¿™äº›å°†åœ¨ initialize_engine ä¸­è¢«å¡«å……
movies_new = None
cv = None
encoder = None
feature = None
similarity = None
G = None
D = None
A = None  # æ¼”å‘˜ç‰¹å¾çŸ©é˜µ
genre_index = None
director_index = None
actor_index = None

# logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# åˆå§‹åŒ–è¿›åº¦çŠ¶æ€ï¼ˆä¾›å¤–éƒ¨ç›‘æ§ï¼‰
init_progress_percent = 0
init_progress_messages = []
init_progress_lock = threading.Lock()

# --- æ··åˆæ¨èæƒé‡é…ç½®ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰ ---
hybrid_weights = {'dvae': 0.6, 'itemcf': 0.4}  # é»˜è®¤æƒé‡ï¼ˆå†…éƒ¨ä»ä¿å­˜ itemcfï¼Œå¤–éƒ¨åªéœ€è®¾ç½® dvaeï¼‰
weights_lock = threading.Lock()

# --- åˆå§‹åŒ–çŠ¶æ€æ ‡å¿—ï¼ˆä¾›å¤–éƒ¨æŸ¥è¯¢ï¼‰ ---
cv_loaded = False
encoder_available = False
encoder_weights_loaded = False
feature_loaded = False
similarity_loaded = False

def get_hybrid_weights():
    """è·å–å½“å‰æ··åˆæ¨èæƒé‡ï¼ˆå¤–éƒ¨åªéœ€å…³å¿ƒ dvaeï¼Œitemcf=1-dvaeï¼‰"""
    with weights_lock:
        dvae = float(hybrid_weights.get('dvae', 0.6))
        itemcf = 1.0 - dvae
        # ä¿è¯æ•°å€¼è¾¹ç•Œ
        dvae = max(0.0, min(1.0, dvae))
        itemcf = max(0.0, min(1.0, itemcf))
        return {'dvae': dvae, 'itemcf': itemcf}

def set_hybrid_weights(dvae_weight=None):
    """è®¾ç½®æ··åˆæ¨èæƒé‡ï¼ˆä»…æ¥å— dvae æƒé‡ï¼‰ï¼Œitemcf ç”± 1-dvae è®¡ç®—ã€‚

    å‚æ•°:
        dvae_weight: DVAE æƒé‡ (0-1)

    è¿”å›: å½’ä¸€åŒ–åçš„æƒé‡å­—å…¸
    """
    global hybrid_weights
    with weights_lock:
        if dvae_weight is not None:
            dvae = max(0.0, min(1.0, float(dvae_weight)))
        else:
            dvae = hybrid_weights.get('dvae', 0.6)

        # è®¡ç®— itemcf ä¸ºè¡¥ä½™
        itemcf = 1.0 - dvae
        hybrid_weights = {'dvae': dvae, 'itemcf': itemcf}
        return {'dvae': dvae, 'itemcf': itemcf}


def get_engine_initialization_status():
    """è¿”å›å¼•æ“åˆå§‹åŒ–çš„è¯¦ç»†çŠ¶æ€ï¼Œä¾›å¤–éƒ¨ï¼ˆä¾‹å¦‚ Flaskï¼‰å±•ç¤ºç»™ç”¨æˆ·ã€‚"""
    global init_progress_percent, init_progress_messages
    status = {
        'progress_percent': int(init_progress_percent),
        'messages': list(init_progress_messages[-20:]) if init_progress_messages else [],
        'cv_loaded': bool(cv is not None),
        'encoder_available': bool(encoder is not None),
        'encoder_weights_loaded': bool(encoder_weights_loaded),
        'feature_loaded': bool(feature is not None),
        'similarity_loaded': bool(similarity is not None),
        'hybrid_weights': get_hybrid_weights(),
    }
    return status

def _set_progress(percent, message=None):
    """çº¿ç¨‹å®‰å…¨åœ°è®¾ç½®è¿›åº¦å’Œé™„åŠ æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘è‹¥å¹²æ¡æ¶ˆæ¯ï¼‰ã€‚"""
    global init_progress_percent, init_progress_messages
    with init_progress_lock:
        try:
            init_progress_percent = int(max(0, min(100, int(percent))))
        except Exception:
            init_progress_percent = 0
        if message:
            init_progress_messages.append(message)
            # é™åˆ¶æ¶ˆæ¯é•¿åº¦ä»¥å…æ— é™å¢é•¿
            if len(init_progress_messages) > 200:
                init_progress_messages = init_progress_messages[-200:]

# æƒé‡æ–‡ä»¶åç»Ÿä¸€
WEIGHTS_FILENAME = 'encoder.weights.h5'

# Note: heavy imports (sklearn, tensorflow) are performed inside functions when needed.
import pandas as pd


# --- è¾…åŠ©å‡½æ•°ï¼šåˆ›å»º CountVectorizer ---
def _get_stopwords():
    """è¿”å›ä¸­æ–‡åœç”¨è¯åˆ—è¡¨"""
    return [
        "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº",
        "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»",
        "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£",
        "ä¸º", "ä¹‹", "å¯¹", "ä¸", "è€Œ", "å¹¶", "ç­‰", "è¢«", "åŠ", "æˆ–",
        "ä½†", "æ‰€ä»¥", "å¦‚æœ", "å› ä¸º", "ç„¶å", "è€Œä¸”", "é‚£ä¹ˆ", "ä»–ä»¬",
        "æˆ‘ä»¬", "ä½ ä»¬", "å®ƒä»¬", "ä»€ä¹ˆ", "å“ªä¸ª", "å“ªäº›", "å“ªé‡Œ", "æ—¶å€™",
        "ä»–", "å¥¹", "å®ƒ", "å’±ä»¬", "å¤§å®¶", "è°", "æ€æ ·", "æ€ä¹ˆ", "å¤šå°‘", "ä¸ºä»€ä¹ˆ",
        "è¿™é‡Œ", "é‚£é‡Œ", "è¿™æ ·", "é‚£æ ·", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›",
        "åœ°", "å¾—", "æ‰€", "è¿‡", "å—", "å‘¢", "å§", "å•Š", "å‘€", "å˜›", "å“‡", "å•¦",
        "ä»", "è‡ª", "ä»¥", "å‘", "å…³äº", "å¯¹äº", "æ ¹æ®", "æŒ‰ç…§", "é€šè¿‡", "ç”±äº",
        "å¹¶ä¸”", "æˆ–è€…", "è™½ç„¶", "å³ä½¿", "å°½ç®¡", "ä¸ç®¡", "åªè¦", "åªæœ‰", "é™¤é",
        "æœ€", "å¤ª", "æ›´", "éå¸¸", "ååˆ†", "ç‰¹åˆ«", "æå…¶", "æ¯”è¾ƒ", "ç¨å¾®", "æœ‰ç‚¹",
        "åˆš", "æ‰", "æ­£åœ¨", "å·²ç»", "æ›¾ç»", "é©¬ä¸Š", "ç«‹åˆ»", "æ°¸è¿œ", "ä¸€ç›´", "æ€»æ˜¯",
        "å¸¸å¸¸", "ç»å¸¸", "å¾€å¾€", "ä¸æ–­", "å¶å°”", "åˆ", "å†", "è¿˜", "ä»…", "å…‰",
        "èƒ½", "èƒ½å¤Ÿ", "å¯ä»¥", "å¯èƒ½", "åº”è¯¥", "åº”å½“", "æƒ³", "æ„¿æ„", "è‚¯", "æ•¢",
        "æ¥", "å»", "è¿›", "å‡º", "å›", "èµ·", "å¼€",
        "äº›", "ä¸€äº›", "æ‰€æœ‰", "æ¯ä¸ª", "æŸä¸ª", "å„ç§", "å¤šä¸ª", "å‡ ä¸ª", "ç¬¬ä¸€", "ç¬¬äºŒ",
        "å°±æ˜¯", "åªæ˜¯", "å¯æ˜¯", "çœŸæ˜¯", "ä¹Ÿæ˜¯", "ä¸æ˜¯", "æ­£æ˜¯",
        "ä¸€æ ·", "ä¸€èˆ¬", "ä¸€ç‚¹", "ä¸€èµ·", "ä¸€ç›´", "ä¸€ä¸‹", "ä¸€ç§", "ä¸€æ¬¡"
    ]


def _create_count_vectorizer():
    """åˆ›å»ºå¹¶è¿”å›é…ç½®å¥½çš„ CountVectorizer å®ä¾‹"""
    stopwords = _get_stopwords()
    # ä½¿ç”¨é¡¶å±‚ tokenizerï¼Œé¿å… lambda å¯¼è‡´ä¸å¯åºåˆ—åŒ–çš„é—®é¢˜
    def jieba_tokenize(text):
        return jieba.lcut(str(text))

    # local import to avoid heavy imports at module import time
    from sklearn.feature_extraction.text import CountVectorizer

    cv = CountVectorizer(
        max_features=10000,
        tokenizer=jieba_tokenize,
        stop_words=stopwords,
        token_pattern=None
    )
    return cv


# ---------------------------
# Genre / Director feature helpers
# ---------------------------
def _extract_tokens_from_series(series, sep_chars=r'[|,;/]'):
    """æŠŠå­—ç¬¦ä¸²åˆ—æ‹†åˆ†æˆ token åˆ—è¡¨"""
    return series.fillna('').astype(str).apply(lambda s: [t.strip() for t in re.split(sep_chars, s) if t.strip()])


def compute_genre_director_actor_features(movies_df, top_k_directors=500, top_k_genres=200, top_k_actors=5000):
    """è®¡ç®—å¹¶ç¼“å­˜å…¨é‡ç”µå½±çš„æµæ´¾ï¼ˆGï¼‰ã€å¯¼æ¼”ï¼ˆDï¼‰ã€æ¼”å‘˜ï¼ˆAï¼‰ç¨€ç–çŸ©é˜µï¼ˆè¡Œå½’ä¸€åŒ–ï¼‰ã€‚

    å…¨å±€èµ‹å€¼ï¼šG, D, A, genre_index, director_index, actor_index
    """
    global G, D, A, genre_index, director_index, actor_index, movies_new
    # å®‰å…¨æ‹·è´ DataFrame æŒ‡é’ˆ
    df = movies_df

    # 1) genres/tags å¤„ç†
    if 'GENRES' in df.columns:
        genres_lists = _extract_tokens_from_series(df['GENRES'], sep_chars=r'[|,;/]')
    elif 'TAGS' in df.columns:
        genres_lists = _extract_tokens_from_series(df['TAGS'], sep_chars=r'[|,;/]')
    else:
        genres_lists = df['INFO'].fillna('').astype(str).apply(lambda s: [])

    from collections import Counter
    all_genres = Counter([g for lst in genres_lists for g in lst])
    top_genres = [g for g, _ in all_genres.most_common(top_k_genres)]
    genre_index = {g: i for i, g in enumerate(top_genres)}

    import numpy as _np
    from sklearn.preprocessing import normalize as _normalize

    G_mat = _np.zeros((len(df), len(top_genres)), dtype=_np.float32)
    for i, lst in enumerate(genres_lists):
        for g in lst:
            if g in genre_index:
                G_mat[i, genre_index[g]] = 1.0
    if G_mat.shape[1] > 0:
        G_mat = _normalize(G_mat, norm='l2', axis=1)
    G = G_mat

    # 2) directors å¤„ç†
    directors_lists = _extract_tokens_from_series(df['DIRECTORS'].fillna(''), sep_chars=r'[/,;]')
    directors_lists = directors_lists.apply(lambda lst: [normalize_text(d) for d in lst])
    all_dirs = Counter([d for lst in directors_lists for d in lst])
    top_dirs = [d for d, _ in all_dirs.most_common(top_k_directors)]
    director_index = {d: i for i, d in enumerate(top_dirs)}

    D_mat = _np.zeros((len(df), len(top_dirs)), dtype=_np.float32)
    for i, lst in enumerate(directors_lists):
        for d in lst:
            if d in director_index:
                D_mat[i, director_index[d]] = 1.0
    if D_mat.shape[1] > 0:
        D_mat = _normalize(D_mat, norm='l2', axis=1)
    D = D_mat

    # 3) actors å¤„ç†
    actors_lists = _extract_tokens_from_series(df['ACTORS'].fillna(''), sep_chars=r'[/,;]')
    actors_lists = actors_lists.apply(lambda lst: [normalize_text(a) for a in lst])
    all_acts = Counter([a for lst in actors_lists for a in lst])
    top_acts = [a for a, _ in all_acts.most_common(top_k_actors)]
    actor_index = {a: i for i, a in enumerate(top_acts)}

    A_mat = _np.zeros((len(df), len(top_acts)), dtype=_np.float32)
    for i, lst in enumerate(actors_lists):
        for a in lst:
            if a in actor_index:
                A_mat[i, actor_index[a]] = 1.0
    if A_mat.shape[1] > 0:
        A_mat = _normalize(A_mat, norm='l2', axis=1)
    A = A_mat

    # åŒæ­¥åˆ°å…¨å±€ movies_newï¼ˆå¦‚æœå°šæœªæŒ‡å‘çš„è¯ï¼‰
    movies_new = df
    return G, D, A, genre_index, director_index, actor_index


def build_user_pref_vectors_from_ids(movie_ids):
    """åŸºäºç”¨æˆ·å–œæ¬¢çš„ movie_idsï¼ˆMOVIE_ID åˆ—ï¼‰æ„å»º U_c/U_g/U_d å‘é‡ã€‚

    è¿”å› dict {'U_c','U_g','U_d'} æˆ– Noneï¼ˆå½“æ— æ³•æ„å»ºæ—¶ï¼‰ã€‚
    """
    global feature, G, D, movies_new
    if movie_ids is None:
        return None
    # æ„é€  id->idx æ˜ å°„
    id_series = movies_new['MOVIE_ID'].astype(str)
    id_to_idx = {v: i for i, v in enumerate(id_series)}
    idxs = [id_to_idx.get(str(mid)) for mid in movie_ids if str(mid) in id_to_idx]
    idxs = [i for i in idxs if i is not None]
    if not idxs:
        return None

    import numpy as _np

    result = {}
    if feature is not None and getattr(feature, 'shape', None):
        try:
            U_c = _np.mean(feature[idxs], axis=0)
            U_c = U_c / (_np.linalg.norm(U_c) + 1e-8)
            result['U_c'] = U_c
        except Exception:
            result['U_c'] = None
    else:
        result['U_c'] = None

    if G is not None and getattr(G, 'shape', None) and G.shape[1] > 0:
        try:
            U_g = _np.mean(G[idxs], axis=0)
            U_g = U_g / (_np.linalg.norm(U_g) + 1e-8)
            result['U_g'] = U_g
        except Exception:
            result['U_g'] = None
    else:
        result['U_g'] = None

    if D is not None and getattr(D, 'shape', None) and D.shape[1] > 0:
        try:
            U_d = _np.mean(D[idxs], axis=0)
            U_d = U_d / (_np.linalg.norm(U_d) + 1e-8)
            result['U_d'] = U_d
        except Exception:
            result['U_d'] = None
    else:
        result['U_d'] = None

    return result


def enhanced_recommend_for_user(movie_name, user_pref_vectors=None, weights=None, sample_top=50, pick_n=15):
    """ç»“åˆå†…å®¹/æµæ´¾/å¯¼æ¼”ä¸ç”¨æˆ·åå¥½å‘é‡çš„å¢å¼ºæ¨èã€‚

    å‚æ•°:
        movie_name: æŸ¥è¯¢ç”µå½±æ ‡é¢˜
        user_pref_vectors: dict from build_user_pref_vectors_from_ids
        weights: æƒé‡å­—å…¸ï¼ˆè§å‡½æ•°å†…é»˜è®¤å€¼ï¼‰
    è¿”å›: pd.DataFrame æ¨èåˆ—è¡¨
    """
    global movies_new, feature, G, D
    # é»˜è®¤æƒé‡
    if weights is None:
        weights = {
            'content': 0.5,
            'genre': 0.15,
            'director': 0.15,
            'user_content': 0.1,
            'user_genre': 0.05,
            'user_director': 0.05,
        }

    # æŸ¥æ‰¾ç”µå½±ç´¢å¼•
    # ç»Ÿä¸€æ ¼å¼åŒ–ç”µå½±å
    norm_movie_name = normalize_text(movie_name)
    norm_names = movies_new['NAME'].fillna('').apply(normalize_text)
    matches = movies_new[norm_names == norm_movie_name]
    if matches.empty:
        # æ¨¡ç³ŠåŒ¹é…ç¬¬ä¸€é¡¹
        similar = movies_new[norm_names.str.contains(norm_movie_name, na=False)]
        if similar.empty:
            return pd.DataFrame()
        q = similar.index[0]
    else:
        q = matches.index[0]

    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†é‡
    n = len(movies_new)
    import numpy as _np
    try:
        from sklearn.metrics.pairwise import cosine_similarity as _cos
    except Exception:
        # sklearn ä¸å¯ç”¨æ—¶é€€åŒ–ä¸ºç©ºå‘é‡
        def _cos(a, b):
            return _np.zeros((a.shape[0], b.shape[0]))

    # content
    if feature is not None and getattr(feature, 'shape', None):
        sims_content = _cos(feature[q:q+1], feature).flatten()
    else:
        sims_content = _np.zeros(n, dtype=_np.float32)

    # genre & director
    sims_genre = _np.zeros(n, dtype=_np.float32)
    sims_director = _np.zeros(n, dtype=_np.float32)
    if G is not None and getattr(G, 'shape', None) and G.shape[1] > 0:
        sims_genre = _cos(G[q:q+1], G).flatten()
    if D is not None and getattr(D, 'shape', None) and D.shape[1] > 0:
        sims_director = _cos(D[q:q+1], D).flatten()

    combined = (_np.zeros(n, dtype=_np.float32)
                + weights.get('content', 0.0) * sims_content
                + weights.get('genre', 0.0) * sims_genre
                + weights.get('director', 0.0) * sims_director)

    # user preference contributions
    if user_pref_vectors is not None:
        U_c = user_pref_vectors.get('U_c')
        U_g = user_pref_vectors.get('U_g')
        U_d = user_pref_vectors.get('U_d')
        if U_c is not None and feature is not None:
            user_content_sim = _cos(feature, U_c.reshape(1, -1)).flatten()
            combined += weights.get('user_content', 0.0) * user_content_sim
        if U_g is not None and G is not None and G.shape[1] > 0:
            user_genre_sim = _cos(G, U_g.reshape(1, -1)).flatten()
            combined += weights.get('user_genre', 0.0) * user_genre_sim
        if U_d is not None and D is not None and D.shape[1] > 0:
            user_dir_sim = _cos(D, U_d.reshape(1, -1)).flatten()
            combined += weights.get('user_director', 0.0) * user_dir_sim

    # æ’é™¤è‡ªèº«
    combined[q] = -1

    top_idxs = _np.argsort(-combined)[:sample_top]
    pick_n = min(pick_n, len(top_idxs))
    if pick_n <= 0:
        return pd.DataFrame()
    picks = _np.random.choice(top_idxs, pick_n, replace=False)

    recs = []
    for idx in picks:
        r = movies_new.iloc[idx]
        recs.append({
            # ä¿æŒä¸æ¨¡æ¿å…¼å®¹çš„å­—æ®µå
            'MOVIE_ID': r.get('MOVIE_ID'),
            'ç”µå½±å': r.get('NAME'),
            'è±†ç“£è¯„åˆ†': r.get('DOUBAN_SCORE'),
            'æµæ´¾': r.get('LABEL') if 'LABEL' in r.index else None,
            'å¯¼æ¼”': r.get('DIRECTORS'),
            'ç›¸ä¼¼åº¦': float(combined[idx])
        })
    return pd.DataFrame(recs)


def build_user_pref_vectors_from_user(user_id):
    """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·å–œæ¬¢çš„ movie_douban_idï¼Œå¹¶æ„å»ºç”¨æˆ·åå¥½å‘é‡ã€‚

    è¯¥å‡½æ•°åœ¨å†…éƒ¨å»¶è¿Ÿå¯¼å…¥ `models` ä»¥é¿å…å¾ªç¯ä¾èµ–ã€‚
    è¿”å›ä¸ build_user_pref_vectors_from_ids ç›¸åŒæ ¼å¼çš„å­—å…¸ï¼Œæˆ– Noneã€‚
    """
    try:
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¼•ç”¨
        from models import UserMoviePreference
    except Exception as e:
        # æ— æ³•å¯¼å…¥ modelsï¼ˆåœ¨æŸäº›æµ‹è¯•åœºæ™¯ï¼‰ï¼Œç›´æ¥è¿”å› None
        print(f"âš ï¸ æ— æ³•å¯¼å…¥ models: {e}")
        return None

    try:
        prefs = UserMoviePreference.query.filter_by(user_id=user_id).all()
        movie_ids = [p.movie_douban_id for p in prefs]
        return build_user_pref_vectors_from_ids(movie_ids)
    except Exception as e:
        print(f"âš ï¸ ä»æ•°æ®åº“æ„å»ºç”¨æˆ·åå¥½å‘é‡å¤±è´¥: {e}")
        return None


# --- ä¸»åˆå§‹åŒ–å‡½æ•° ---
# å‡è®¾ movies_new, encoder, feature, similarity, _build_encoder_structure æ˜¯åœ¨æ¨¡å—çº§åˆ«å®šä¹‰çš„å…¨å±€å˜é‡æˆ–å‡½æ•°
# from somewhere import movies_new, encoder, feature, similarity, _build_encoder_structure

def initialize_engine(data_folder_path, model_cache_path="model_cache.pkl"):
    # è°ƒè¯•ï¼šè¾“å‡ºæ¼”å‘˜ç´¢å¼•æ•°é‡å’Œå‰10ä¸ªæ¼”å‘˜å
    import sys
    def _debug_actor_index():
        global actor_index
        if actor_index:
            print(f"[DEBUG] æ¼”å‘˜ç´¢å¼•æ•°é‡: {len(actor_index)}", file=sys.stderr)
            print(f"[DEBUG] æ¼”å‘˜æ ·ä¾‹: {list(actor_index.keys())[:10]}", file=sys.stderr)
        else:
            print("[DEBUG] æ¼”å‘˜ç´¢å¼•ä¸ºç©ºï¼", file=sys.stderr)

    """
    åˆå§‹åŒ–æ¨èå¼•æ“ï¼šåŠ è½½æ•°æ®ã€é¢„å¤„ç†ã€è®­ç»ƒDVAEæ¨¡å‹ï¼ˆå¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼‰ã€‚
    """
    # å£°æ˜éœ€è¦ä¿®æ”¹çš„å…¨å±€å˜é‡
    global movies_new, cv, encoder, feature, similarity
    # æ³¨æ„ï¼š'encoder' åªéœ€å£°æ˜ä¸€æ¬¡ï¼Œå¦‚æœä¹‹å‰å·²å£°æ˜è¿‡ï¼Œè¯·åˆ é™¤é‡å¤çš„ global encoder

    # local imports of heavy libs to avoid module-level import cost
    # Use tolerant imports so cached-only startup can succeed without TF
    try:
        import tensorflow as tf
        from tensorflow import keras
    except Exception as _e:
        tf = None
        keras = None
        print(f"âš ï¸ tensorflow import failed or unavailable: {_e}")
    try:
        from sklearn.metrics.pairwise import cosine_similarity
    except Exception as _e:
        cosine_similarity = None
        print(f"âš ï¸ sklearn.metrics.pairwise.cosine_similarity import failed: {_e}")

    cache_exists = os.path.exists(model_cache_path)
    _set_progress(1, "å¼€å§‹åˆå§‹åŒ–ï¼šæ£€æŸ¥ç¼“å­˜")
    if cache_exists:
        _set_progress(5, "æ£€æµ‹åˆ° model_cacheï¼Œå°è¯•ä»ç¼“å­˜åŠ è½½...")
        print("ğŸ” å°è¯•ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ¨¡å‹å’Œç‰¹å¾...")
        try:
            with open(model_cache_path, 'rb') as f:
                cache = pickle.load(f)
                # å°½é‡ä»ç¼“å­˜æ¢å¤ä¸ä¾èµ– heavy lib çš„æ•°æ®ï¼ˆmovies_new/feature/similarityï¼‰
                movies_new = cache.get('movies_new', None)
                feature = cache.get('feature', None)
                similarity = cache.get('similarity', None)

                # å°è¯•æ¢å¤æµæ´¾/å¯¼æ¼”ç‰¹å¾ï¼ˆå¦‚æœç¼“å­˜ä¸­å­˜åœ¨ï¼‰
                try:
                    G = cache.get('G', None)
                    D = cache.get('D', None)
                    genre_index = cache.get('genre_index', None)
                    director_index = cache.get('director_index', None)
                except Exception:
                    G = D = genre_index = director_index = None

                # å°è¯•æ¢å¤ director_to_label
                try:
                    director_to_label = cache.get('director_to_label', None)
                except Exception:
                    director_to_label = None

                # é‡æ–°åˆ›å»º CountVectorizer å¹¶æ¢å¤è¯è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    cv = _create_count_vectorizer()
                    vocab = cache.get('cv_vocab', None)
                    if vocab is not None:
                        cv.vocabulary_ = vocab
                except Exception:
                    cv = _create_count_vectorizer()

                # å°è¯•æ„å»º encoder å¹¶åŠ è½½æƒé‡ï¼›è‹¥ç³»ç»Ÿç¼ºå°‘ tensorflowï¼Œåˆ™è·³è¿‡ä½†ä¸é˜»å¡åˆå§‹åŒ–
                try:
                    _build_encoder_structure(cache.get('inp_dim'), cache.get('code_dim'))
                    try:
                        encoder.load_weights(os.path.join(os.path.dirname(model_cache_path), WEIGHTS_FILENAME))
                    except Exception as e:
                        # æƒé‡åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½ç¼ºå°‘æ–‡ä»¶æˆ–ä¸å…¼å®¹ï¼‰ï¼Œè®°å½•ä½†ç»§ç»­
                        print(f"âš ï¸ è½½å…¥ encoder æƒé‡å¤±è´¥: {e}")
                except Exception as e:
                    # å¦‚æœ tensorflow ä¸å¯ç”¨æˆ–æ„å»ºå¤±è´¥ï¼Œè®°å½•å¹¶ç»§ç»­ï¼ˆä¸é˜»å¡ï¼‰
                    print(f"âš ï¸ æ— æ³•é‡å»º encoderï¼ˆå¯èƒ½ç¼ºå°‘ tensorflowï¼‰ï¼š{e}")

                _set_progress(100, "æˆåŠŸä»ç¼“å­˜åŠ è½½å®Œæˆï¼ˆéƒ¨åˆ†åŠŸèƒ½å¯èƒ½è¢«é™çº§ï¼‰")
                print("âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½ï¼ˆéƒ¨åˆ†åŠŸèƒ½å¯èƒ½è¢«é™çº§ï¼‰!")
                return
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°è®¡ç®—...")

    _set_progress(10, "ç¼“å­˜ä¸å¯ç”¨æˆ–åŠ è½½å¤±è´¥ï¼Œå¼€å§‹é¢„å¤„ç†æ•°æ®å’Œè®­ç»ƒæ¨¡å‹")
    print("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®å’Œè®­ç»ƒæ¨¡å‹...")

    # 1. è¯»å…¥åŸå§‹æ•°æ®
    movies_path = os.path.join(data_folder_path, "movies.csv")
    movies_db_path = os.path.join(data_folder_path, "movies_db.csv")
    director_label_path = os.path.join(data_folder_path, "director_label.csv")

    _set_progress(12, "è¯»å– CSV æ•°æ®")
    movies = pd.read_csv(movies_path)
    movies_db = pd.read_csv(movies_db_path)

    # 2. æ¸…æ´— movies_dbï¼Œæ„é€  INFO
    movies_db = movies_db.drop(columns=["durations", "votes"])
    movies_db["INFO"] = (
        movies_db["genres"].fillna("").astype(str) + " " +
        movies_db["countries"].fillna("").astype(str) + " " +
        movies_db["reviews"].fillna("").astype(str)
    )
    movies_db = movies_db.drop(columns=["genres", "countries", "reviews"])
    movies_db["title"] = movies_db["title"].apply(
        lambda x: "".join(re.findall(r"[\u4e00-\u9fff]+", str(x)))
    )

    # 3. æ¸…æ´— moviesï¼Œæœ¬ä½“åªä¿ç•™é«˜åˆ†ç”µå½±ï¼Œä¿ç•™æ¼”å‘˜ä¿¡æ¯
    movies = movies.drop(
        columns=[
            "COVER", "IMDB_ID", "MINS", "OFFICIAL_SITE", "RELEASE_DATE",
            "SLUG", "ACTOR_IDS", "DIRECTOR_IDS", "LANGUAGES", "GENRES",
            "ALIAS"  # æ³¨æ„ï¼šä¸å† drop "ACTORS"
        ]
    )
    movies = movies[movies["DOUBAN_SCORE"] >= 6.5]

    # 4. æ„é€  movies_newï¼ˆè¯„åˆ†/äººæ•°è¿‡æ»¤ï¼‰ï¼Œä¿ç•™æ¼”å‘˜ä¿¡æ¯
    movies_new_filtered = movies[movies["DOUBAN_VOTES"] >= 3000] \
        .sort_values(by=["DOUBAN_SCORE", "DOUBAN_VOTES"], ascending=[False, False])[
        ["DIRECTORS", "ACTORS", "MOVIE_ID", "NAME", "DOUBAN_SCORE",
         "STORYLINE", "TAGS", "REGIONS", "YEAR"]
    ]

    # 5. æ‹¼æ¥å‰§æƒ… + æ ‡ç­¾ + åœ°åŒº ä½œä¸º INFO
    movies_new_filtered["INFO"] = (
        movies_new_filtered["STORYLINE"].fillna("").astype(str) + " " +
        movies_new_filtered["TAGS"].fillna("").astype(str) + " " +
        movies_new_filtered["REGIONS"].fillna("").astype(str)
    )
    movies_new_filtered = movies_new_filtered.drop(columns=["STORYLINE", "TAGS", "REGIONS"])

    # 6. æ‹¼æ¥ movies_dbï¼ˆçˆ¬è™«æ¥çš„æ•°æ®ï¼‰ï¼Œä¿ç•™æ¼”å‘˜ä¿¡æ¯
    movies_db_renamed = movies_db.rename(columns={
        "subject_id": "MOVIE_ID",
        "title": "NAME",
        "year": "YEAR",
        "rating": "DOUBAN_SCORE",
        "directors": "DIRECTORS",
        "actors": "ACTORS"
    })
    # æŸäº›çˆ¬è™«æ•°æ®å¯èƒ½æ²¡æœ‰ actors å­—æ®µï¼Œéœ€é˜²å¾¡æ€§å¤„ç†
    db_cols = ["DIRECTORS", "MOVIE_ID", "NAME", "DOUBAN_SCORE", "YEAR", "INFO"]
    if "ACTORS" in movies_db_renamed.columns:
        db_cols.insert(1, "ACTORS")
    movies_db_renamed = movies_db_renamed[db_cols]

    # 7. åˆå¹¶ä¸¤éƒ¨åˆ†æ•°æ®
    movies_new_combined = pd.concat([movies_new_filtered, movies_db_renamed], ignore_index=True)

    # 8. åŠ å¯¼æ¼”æ ‡ç­¾
    director_label = pd.read_csv(director_label_path)
    director_to_label = dict(zip(director_label["DIRECTOR"], director_label["LABEL"]))
    movies_new_combined["LABEL"] = movies_new_combined["DIRECTORS"].apply(
        lambda x: ",".join(
            {
                director_to_label.get(d.strip())
                for d in str(x).split("/")
                if director_to_label.get(d.strip())
            }
        ) if pd.notna(x) else None
    )

    # æ›´æ–°å…¨å±€å˜é‡ movies_new
    movies_new = movies_new_combined
    _set_progress(30, "æ•°æ®æ¸…æ´—å®Œæˆ")
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")

    # é¢„è®¡ç®—æµæ´¾å’Œå¯¼æ¼”ç‰¹å¾ï¼Œä¾¿äºåç»­ä¸ªæ€§åŒ–æ¨è
    try:
        _set_progress(40, "æ­£åœ¨è®¡ç®—æµæ´¾/å¯¼æ¼”/æ¼”å‘˜ç‰¹å¾")
        compute_genre_director_actor_features(movies_new)
        _debug_actor_index()
        _set_progress(50, "æµæ´¾/å¯¼æ¼”/æ¼”å‘˜ç‰¹å¾é¢„è®¡ç®—å®Œæˆ")
        print("âœ… æµæ´¾/å¯¼æ¼”/æ¼”å‘˜ç‰¹å¾é¢„è®¡ç®—å®Œæˆ")
    except Exception as e:
        _set_progress(45, f"æµæ´¾/å¯¼æ¼”/æ¼”å‘˜ç‰¹å¾é¢„è®¡ç®—å¤±è´¥: {e}")
        print(f"âš ï¸ æµæ´¾/å¯¼æ¼”/æ¼”å‘˜ç‰¹å¾é¢„è®¡ç®—å¤±è´¥: {e}")

    # --- BOW + DVAE ---
    # ä½¿ç”¨è¾…åŠ©å‡½æ•°åˆ›å»º CountVectorizer
    cv = _create_count_vectorizer()

    vector = cv.fit_transform(movies_new["INFO"].astype(str)).toarray().astype("float32")
    _set_progress(60, "BOW å‘é‡æ„å»ºå®Œæˆ")
    print("âœ… BOW å‘é‡æ„å»ºå®Œæˆ")

    # DVAE å‚æ•°
    inp_dim = vector.shape[1]
    code_dim = 64
    epochs = 5  # è°ƒè¯•é˜¶æ®µè®¾å°ï¼Œç”Ÿäº§å¯è°ƒå¤§
    batch_size = 256
    beta_kl = 1.0

    # ç¼–ç å™¨
    inputs = keras.Input(shape=(inp_dim,), name="bow_counts")
    x = keras.layers.GaussianNoise(0.15)(inputs)
    x = keras.layers.Dense(1000, activation="selu")(x)
    x = keras.layers.Dense(256, activation="selu")(x)
    z_mean = keras.layers.Dense(code_dim, name="z_mean")(x)
    z_logvar = keras.layers.Dense(code_dim, name="z_logvar")(x)

    def reparameterize(args):
        mu, logvar = args
        eps = tf.random.normal(shape=tf.shape(mu))
        return mu + tf.exp(0.5 * logvar) * eps

    z = keras.layers.Lambda(reparameterize, name="z")([z_mean, z_logvar])
    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name="dvae_encoder")

    # è§£ç å™¨ (ç”¨äºè®­ç»ƒ)
    latent_inputs = keras.Input(shape=(code_dim,), name="z_in")
    d = keras.layers.Dense(256, activation="selu")(latent_inputs)
    d = keras.layers.Dense(1000, activation="selu")(d)
    recons = keras.layers.Dense(inp_dim, activation=None, name="recon")(d)
    decoder = keras.Model(latent_inputs, recons, name="dvae_decoder")

    # KL æ­£åˆ™å±‚
    class KLDivergenceLayer(keras.layers.Layer):
        def __init__(self, beta=1.0, scale=1.0, **kwargs):
            super().__init__(**kwargs)
            self.beta = beta
            self.scale = scale

        def call(self, inputs):
            mu, logvar = inputs
            kl_per_sample = -0.5 * tf.reduce_sum(
                1.0 + logvar - tf.exp(logvar) - tf.square(mu), axis=1
            )
            kl = tf.reduce_mean(kl_per_sample) / float(self.scale)
            self.add_loss(self.beta * kl)
            return tf.zeros_like(mu[:, :1])

    z_mean_out, z_logvar_out, z_out = encoder(inputs)
    _ = KLDivergenceLayer(beta=beta_kl, scale=inp_dim, name="kl_reg")(
        [z_mean_out, z_logvar_out]
    )
    recons_out = decoder(z_out)

    vae = keras.Model(inputs, recons_out, name="dvae")
    vae.compile(optimizer=keras.optimizers.Adam(1e-3), loss="mse")

    # è®­ç»ƒ VAE
    _set_progress(65, "å¼€å§‹è®­ç»ƒ DVAEï¼ˆå¯èƒ½è€—æ—¶ï¼‰")
    history = vae.fit(
        vector, vector,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.1,
        verbose=1,
    )
    _set_progress(85, "DVAE æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print("âœ… DVAE æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # æå–ç”µå½±è¯­ä¹‰å‘é‡ featureï¼ˆz_meanï¼‰
    z_mean_val = encoder.predict(vector, verbose=0)[0]
    feature = z_mean_val.astype("float32")
    _set_progress(88, "ç”µå½±è¯­ä¹‰ç‰¹å¾æå–å®Œæˆ")
    print("âœ… ç”µå½±è¯­ä¹‰ç‰¹å¾æå–å®Œæˆ")

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity = cosine_similarity(feature)
    _set_progress(94, "ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ")
    print("âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ")

    # --- ç¼“å­˜æ¨¡å‹å’Œç‰¹å¾ ---
    print("ğŸ’¾ æ­£åœ¨ç¼“å­˜æ¨¡å‹å’Œç‰¹å¾...")
    # æ³¨æ„ï¼šä¸å†ç¼“å­˜ 'cv' å¯¹è±¡ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸å¯ pickle çš„ lambda
    cache_to_save = {
        'movies_new': movies_new,     # DataFrame
        # 'cv': cv,                   # <-- ç§»é™¤æ­¤è¡Œ
        'feature': feature,           # NumPy array
        'similarity': similarity,     # NumPy array
        'inp_dim': inp_dim,           # int (ç”¨äºé‡å»º encoder ç»“æ„)
        'code_dim': code_dim          # int (ç”¨äºé‡å»º encoder ç»“æ„)
        # å¦‚æœéœ€è¦ç¼“å­˜ director_to_labelï¼Œä¹Ÿå¯ä»¥åŠ ä¸Š
        # 'director_to_label': director_to_label 
    }
    
    try:
        # è¡¥å……è¦ç¼“å­˜çš„ G/D/indices ä¸ cv vocabulary
        try:
            cache_to_save['G'] = G
            cache_to_save['D'] = D
            cache_to_save['genre_index'] = genre_index
            cache_to_save['director_index'] = director_index
        except Exception:
            pass
        try:
            # ä¿å­˜ cv çš„ vocabularyï¼ˆå¯ç”¨äºå¿«é€Ÿæ¢å¤ CountVectorizer çš„è¯è¡¨ï¼‰
            if cv is not None and hasattr(cv, 'vocabulary_'):
                cache_to_save['cv_vocab'] = cv.vocabulary_
        except Exception:
            pass

        with open(model_cache_path, 'wb') as f:
            pickle.dump(cache_to_save, f)
        encoder.save_weights(os.path.join(os.path.dirname(model_cache_path), WEIGHTS_FILENAME))
        _set_progress(98, "ç¼“å­˜ä¿å­˜æˆåŠŸ")
        print("âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ!")
    except Exception as e:
        _set_progress(97, f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        # æ ¹æ®ä½ çš„éœ€æ±‚å†³å®šæ˜¯å¦è¦åœ¨è¿™é‡ŒæŠ›å‡ºå¼‚å¸¸
        # raise e # å¦‚æœç¼“å­˜å¤±è´¥æ˜¯è‡´å‘½é”™è¯¯ï¼Œå–æ¶ˆæ³¨é‡Šæ­¤è¡Œ

    # æ³¨æ„ï¼šå‡½æ•°ç»“æŸï¼Œcv å·²åœ¨æ­¤å‡½æ•°ä½œç”¨åŸŸå†…åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡
    _set_progress(100, "åˆå§‹åŒ–å®Œæˆ")

def _build_encoder_structure(inp_dim, code_dim):
    """é‡å»ºç¼–ç å™¨ç»“æ„ä»¥ä¾¿åŠ è½½æƒé‡"""
    global encoder
    # local import to avoid top-level dependency
    import tensorflow as tf
    from tensorflow import keras

    inputs = keras.Input(shape=(inp_dim,), name="bow_counts")
    x = keras.layers.GaussianNoise(0.15)(inputs)
    x = keras.layers.Dense(1000, activation="selu")(x)
    x = keras.layers.Dense(256, activation="selu")(x)
    z_mean = keras.layers.Dense(code_dim, name="z_mean")(x)
    z_logvar = keras.layers.Dense(code_dim, name="z_logvar")(x)

    def reparameterize(args):
        mu, logvar = args
        eps = tf.random.normal(shape=tf.shape(mu))
        return mu + tf.exp(0.5 * logvar) * eps

    z = keras.layers.Lambda(reparameterize, name="z")([z_mean, z_logvar])
    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name="dvae_encoder")


def get_movie_features():
    """è·å–ç”µå½±ç‰¹å¾å‘é‡"""
    return feature


def get_movies_dataframe():
    """è·å–å¤„ç†åçš„ç”µå½±DataFrame"""
    return movies_new


def get_similarity_matrix():
    """è·å–ç”µå½±ç›¸ä¼¼åº¦çŸ©é˜µ"""
    return similarity


def recommand(movie_name, sample_top=15, pick_n=5):
    """åŸºç¡€æ¨èå‡½æ•°ï¼ˆåªç”¨å†…å®¹ç›¸ä¼¼ï¼‰"""
    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿æ•°æ®å·²åˆå§‹åŒ–
    if movies_new is None or similarity is None:
        logger.warning('recommand called before engine initialized or data missing')
        return pd.DataFrame()
    label_idx = movies_new.index[movies_new["NAME"] == movie_name]
    if len(label_idx) == 0:
        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        similar_movies = movies_new[movies_new["NAME"].str.contains(movie_name, na=False, case=False)]
        if len(similar_movies) > 0:
            print(f"æœªç²¾ç¡®æ‰¾åˆ°ã€Š{movie_name}ã€‹ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…:")
            for idx, row in similar_movies.head(3).iterrows():
                 print(f"  - {row['NAME']}")
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
            pos = similar_movies.index[0]
        else:
            print(f"æœªæ‰¾åˆ°å½±ç‰‡ï¼šã€Š{movie_name}ã€‹")
            return None
    else:
        pos = movies_new.index.get_loc(label_idx[0])

    sims = similarity[pos]
    cand = np.argsort(-sims)  # é™åº
    cand = cand[cand != pos]  # å»æ‰è‡ªèº«
    top_candidates = cand[:sample_top]

    n_pick = min(pick_n, len(top_candidates))
    if n_pick == 0:
        return pd.DataFrame()
    selected = np.random.choice(top_candidates, n_pick, replace=False)

    recs = []
    for j in selected:
        row = movies_new.iloc[j]
        recs.append({
            "MOVIE_ID": row.get('MOVIE_ID'),
            "ç”µå½±å": row["NAME"],
            "è±†ç“£è¯„åˆ†": row["DOUBAN_SCORE"],
            "æµæ´¾": row.get("LABEL"),
            "ç›¸ä¼¼åº¦": sims[j],
            "å¯¼æ¼”": row.get("DIRECTORS"),
        })
    df = pd.DataFrame(recs).sort_values(by="ç›¸ä¼¼åº¦", ascending=False).reset_index(drop=True)
    return df


def itemcf_recommend_for_movie(movie_name, sample_top=50):
    """åŸºäºå…±ç°çš„ ItemCFï¼šå¯¹ç»™å®šç”µå½±è®¡ç®—ä¸å…¶å®ƒç”µå½±çš„ååŒè¿‡æ»¤ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

    è¿”å›é•¿åº¦ä¸º n çš„ numpy æ•°ç»„ï¼ˆä¸ `movies_new` è¡Œå¯¹åº”ï¼‰ï¼Œæ•°å€¼ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆfloatï¼‰ã€‚
    å½“æ•°æ®åº“æˆ–æ•°æ®ä¸è¶³æ—¶è¿”å›å…¨é›¶å‘é‡ã€‚
    """
    global movies_new
    n = len(movies_new) if movies_new is not None else 0
    if n == 0:
        return np.zeros(0, dtype=np.float32)

    # æ‰¾åˆ°æŸ¥è¯¢ç”µå½±ç´¢å¼•ï¼ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼Œå†æ¨¡ç³ŠåŒ¹é…ï¼‰
    matches = movies_new[movies_new['NAME'] == movie_name]
    if matches.empty:
        similar = movies_new[movies_new['NAME'].str.contains(movie_name, na=False, case=False)]
        if similar.empty:
            return np.zeros(n, dtype=np.float32)
        target_idx = similar.index[0]
    else:
        target_idx = matches.index[0]

    # å»¶è¿Ÿå¯¼å…¥ modelsï¼Œé¿å…å¾ªç¯ä¾èµ–
    try:
        from models import UserMoviePreference
    except Exception:
        return np.zeros(n, dtype=np.float32)

    # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç”¨æˆ·-å–œæ¬¢å…³ç³»
    try:
        prefs = UserMoviePreference.query.with_entities(UserMoviePreference.user_id, UserMoviePreference.movie_douban_id).all()
    except Exception:
        return _np.zeros(n, dtype=_np.float32)

    # æ„å»º movie -> set(users) æ˜ å°„
    movie_users = {}
    for uid, mid in prefs:
        movie_users.setdefault(str(mid), set()).add(int(uid))

    # ç›®æ ‡ç”µå½±çš„ MOVIE_ID
    target_mid = str(movies_new.at[target_idx, 'MOVIE_ID'])
    users_target = movie_users.get(target_mid, set())
    if not users_target:
        return np.zeros(n, dtype=np.float32)

    # è®¡ç®—ä¸æ¯ä¸ª movie çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…±ç° / cosine-likeï¼‰
    scores = np.zeros(n, dtype=np.float32)
    for i in range(n):
        try:
            mid_i = str(movies_new.at[i, 'MOVIE_ID'])
        except Exception:
            continue
        users_i = movie_users.get(mid_i, set())
        if not users_i:
            continue
        inter = len(users_target & users_i)
        if inter == 0:
            continue
        # cosine-like normalization
        denom = (np.sqrt(len(users_target) * len(users_i)))
        if denom > 0:
            scores[i] = float(inter) / float(denom)

    # æ’é™¤è‡ªèº«
    scores[target_idx] = -1.0
    return scores


def hybrid_recommend_for_user(movie_name, user_id=None, weights=None, sample_top=50, pick_n=15):
    """æ··åˆæ¨èï¼šå°† DVAE(content) ç›¸ä¼¼åº¦ä¸ itemCF å…±ç°åˆ†æ•°æŒ‰æƒé‡åˆå¹¶å¹¶è¿”å›æ¨è DataFrameã€‚

    å‚æ•°:
        movie_name: æŸ¥è¯¢ç”µå½±åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
        user_id: å¯é€‰çš„ç”¨æˆ· idï¼ˆå½“å‰å®ç°æœªç›´æ¥ç”¨åˆ°ï¼Œä½†ä¿ç•™æ¥å£å…¼å®¹ï¼‰
        weights: dict, e.g. {'dvae':0.6, 'itemcf':0.4}
    è¿”å›: pd.DataFrame ä¸ `recommand` / `enhanced_recommend_for_user` ç›¸åŒæ ¼å¼
    """
    global similarity, movies_new
    if weights is None:
        weights = {'dvae': 0.6, 'itemcf': 0.4}

    n = len(movies_new) if movies_new is not None else 0
    if n == 0:
        return pd.DataFrame()

    # 1) DVAE ç›¸ä¼¼åº¦åˆ†é‡
    try:
        # å¯»æ‰¾æŸ¥è¯¢ç”µå½±ç´¢å¼•
        matches = movies_new[movies_new['NAME'] == movie_name]
        if matches.empty:
            similar = movies_new[movies_new['NAME'].str.contains(movie_name, na=False, case=False)]
            if similar.empty:
                return pd.DataFrame()
            q = similar.index[0]
        else:
            q = matches.index[0]

        if similarity is None:
            sims_dvae = np.zeros(n, dtype=np.float32)
        else:
            sims_dvae = np.array(similarity[q], dtype=np.float32)
    except Exception:
        sims_dvae = np.zeros(n, dtype=np.float32)

    # 2) itemCF åˆ†é‡ï¼ˆå…±ç°ï¼‰
    try:
        sims_item = itemcf_recommend_for_movie(movie_name, sample_top=sample_top)
        if sims_item.shape[0] != n:
            sims_item = np.zeros(n, dtype=np.float32)
    except Exception:
        sims_item = np.zeros(n, dtype=np.float32)

    # å½’ä¸€åŒ–ä¸¤ä¸ªåˆ†é‡ï¼ˆé¿å…å°ºåº¦å·®å¼‚ï¼‰
    def _normalize_vec(v):
        vmax = v.max() if v.size > 0 else 0.0
        vmin = v.min() if v.size > 0 else 0.0
        if vmax - vmin > 1e-9:
            return (v - vmin) / (vmax - vmin)
        return v

    nd = _normalize_vec(sims_dvae)
    ni = _normalize_vec(sims_item)

    combined = weights.get('dvae', 0.0) * nd + weights.get('itemcf', 0.0) * ni

    # æ’é™¤æŸ¥è¯¢è‡ªèº«ï¼ˆå¦‚æœèƒ½å®šä½åˆ°ï¼‰
    try:
        if 'q' in locals():
            combined[q] = -1.0
    except Exception:
        pass

    # å– top
    top_idxs = np.argsort(-combined)[:sample_top]
    pick_n = min(pick_n, len(top_idxs))
    if pick_n <= 0:
        return pd.DataFrame()
    picks = np.random.choice(top_idxs, pick_n, replace=False)

    recs = []
    for idx in picks:
        r = movies_new.iloc[idx]
        recs.append({
            'MOVIE_ID': r.get('MOVIE_ID'),
            'ç”µå½±å': r.get('NAME'),
            'è±†ç“£è¯„åˆ†': r.get('DOUBAN_SCORE'),
            'æµæ´¾': r.get('LABEL') if 'LABEL' in r.index else None,
            'å¯¼æ¼”': r.get('DIRECTORS'),
            'ç›¸ä¼¼åº¦': float(combined[idx])
        })
    return pd.DataFrame(recs)


def get_popular_movies(data_folder_path=None, count=20, min_score=8.5, min_votes=100000):
    """
    è·å–çƒ­é—¨ç”µå½±ï¼šéšæœºé€‰æ‹©æ»¡è¶³æ¡ä»¶çš„ç”µå½±
    
    Args:
        data_folder_path (str): æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¦‚æœ Noneï¼Œä½¿ç”¨å…¨å±€ movies_newï¼‰
        count (int): è¿”å›ç”µå½±æ•°é‡ï¼Œé»˜è®¤ 20
        min_score (float): æœ€ä½è±†ç“£è¯„åˆ†ï¼Œé»˜è®¤ 8.5
        min_votes (int): æœ€ä½è¯„åˆ†äººæ•°ï¼Œé»˜è®¤ 100000
    
    Returns:
        pd.DataFrame: åŒ…å«çƒ­é—¨ç”µå½±çš„ DataFrameï¼Œæˆ–ç©º DataFrame
    """
    global movies_new
    
    # å¦‚æœå·²åˆå§‹åŒ–ä¸”æœ‰æ•°æ®ï¼Œå°è¯•ä»å…¨å±€ movies_new ä¸­è·å–
    if movies_new is not None and not movies_new.empty:
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        if 'DOUBAN_SCORE' in movies_new.columns:
            # ä»å…¨å±€æ•°æ®ä¸­ç­›é€‰
            # å…ˆç¡®ä¿è¯„åˆ†å’ŒæŠ•ç¥¨æ•°ä¸ºæ•°å€¼ç±»å‹ï¼Œé˜²æ­¢å­—ç¬¦ä¸²æ¯”è¾ƒå¯¼è‡´ TypeError
            df = movies_new.copy()
            if 'DOUBAN_SCORE' in df.columns:
                df['DOUBAN_SCORE'] = pd.to_numeric(df['DOUBAN_SCORE'], errors='coerce')
            if 'DOUBAN_VOTES' in df.columns:
                df['DOUBAN_VOTES'] = pd.to_numeric(df['DOUBAN_VOTES'], errors='coerce')

            # æ„é€ è¿‡æ»¤æ¡ä»¶ï¼Œä½¿ç”¨ .ge/.ge ä»¥é¿å…ç±»å‹ä¸ä¸€è‡´çš„æ¯”è¾ƒ
            cond = df['DOUBAN_SCORE'].ge(min_score)
            if 'DOUBAN_VOTES' in df.columns:
                cond = cond & df['DOUBAN_VOTES'].ge(min_votes)

            popular = df[cond].copy()
            
            if not popular.empty:
                # éšæœºé€‰æ‹© count éƒ¨ç”µå½±
                sample_count = min(count, len(popular))
                result = popular.sample(n=sample_count, random_state=None).reset_index(drop=True)
                logger.info(f"ä»å…¨å±€ movies_new ä¸­è·å– {sample_count} éƒ¨çƒ­é—¨ç”µå½±")
                return result
    
    # å¦‚æœæ²¡æœ‰åˆå§‹åŒ–æˆ–å…¨å±€æ•°æ®ä¸è¶³ï¼Œä»åŸå§‹ CSV è¯»å–
    if data_folder_path is None:
        logger.warning("æ— æ³•è·å–çƒ­é—¨ç”µå½±ï¼šmovies_new æœªåˆå§‹åŒ–ä¸”æœªæä¾› data_folder_path")
        return pd.DataFrame()
    
    movies_csv_path = os.path.join(data_folder_path, 'movies.csv')
    if not os.path.exists(movies_csv_path):
        logger.error(f"movies.csv ä¸å­˜åœ¨: {movies_csv_path}")
        return pd.DataFrame()
    
    try:
        # ç›´æ¥ä» CSV è¯»å–åŸå§‹æ•°æ®
        movies = pd.read_csv(movies_csv_path)
        
        # ç­›é€‰æ¡ä»¶ï¼šå…ˆå°†è¯„åˆ†å’ŒæŠ•ç¥¨æ•°è½¬æ¢æˆæ•°å€¼å‹ï¼Œå†åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        if 'DOUBAN_SCORE' in movies.columns:
            movies['DOUBAN_SCORE'] = pd.to_numeric(movies['DOUBAN_SCORE'], errors='coerce')
        if 'DOUBAN_VOTES' in movies.columns:
            movies['DOUBAN_VOTES'] = pd.to_numeric(movies['DOUBAN_VOTES'], errors='coerce')

        popular = movies[
            (movies['DOUBAN_SCORE'].ge(min_score)) & 
            (movies['DOUBAN_VOTES'].ge(min_votes))
        ].copy()
        
        if popular.empty:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç”µå½± (è¯„åˆ†>={min_score}, è¯„åˆ†äººæ•°>={min_votes})")
            return pd.DataFrame()
        
        # éšæœºé€‰æ‹© count éƒ¨ç”µå½±
        sample_count = min(count, len(popular))
        result = popular.sample(n=sample_count, random_state=None).reset_index(drop=True)
        
        # åªä¿ç•™éœ€è¦æ˜¾ç¤ºçš„åˆ—
        needed_cols = ['MOVIE_ID', 'NAME', 'DOUBAN_SCORE', 'DOUBAN_VOTES', 'YEAR', 'DIRECTORS']
        available_cols = [col for col in needed_cols if col in result.columns]
        result = result[available_cols]
        
        logger.info(f"ä» CSV ä¸­è·å– {sample_count} éƒ¨çƒ­é—¨ç”µå½± (è¯„åˆ†>={min_score}, è¯„åˆ†äººæ•°>={min_votes})")
        return result
    
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨ç”µå½±æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()